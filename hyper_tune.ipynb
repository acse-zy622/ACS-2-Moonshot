{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameters we have used are all in google onedrive. It should be in the path of './acds_project/hyper_result.zip'. As the result of grid search, we set the learning rate to be 0.001. what is more, we finally choose the optimizer AdamW, which is a optimization strategy applying stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments with an added method to decay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2b5b5811a8a1eac832bf802133e6f07ede537aed9dd63053f40db53c079a83cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
